{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: ATE Estimation with the voter turnout data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we'll be working with some of the data described in the paper [“Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment”](http://isps.yale.edu/sites/default/files/publication/2012/12/ISPS08-001.pdf) by Gerber et al. (2008).  We attained the data from this [Github repo](https://github.com/gsbDBI/ExperimentData/tree/master/Social), specifically this [file](https://raw.githubusercontent.com/gsbDBI/ExperimentData/master/Social/ProcessedData/socialpresswgeooneperhh_NEIGH.csv). It's also included in the assignment zip file.  In this assignment we'll build several ATE estimators by reduction to two estimations from incomplete data, as discussed in lecture. Parts of this notebook are based on a [notebook](https://gsbdbi.github.io/ml_tutorial/ate_tutorial/ate_tutorial.html) from a tutorial/course on causal inference at Stanford GSB.  Although not necessary, you may find it interesting to refer to, as they give more details about the covariates, and they they cover some methods that we don't get into (and vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from scipy.stats import norm, sem\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.stats import pearsonr\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zYCddcESYhE6"
   },
   "outputs": [],
   "source": [
    "## Read the data, select some columns, and lightly process\n",
    "df = pd.read_csv('sp.csv.xz')\n",
    "cts_variables_names = [\"yob\", \"hh_size\", \"totalpopulation_estimate\",\n",
    "                         \"percent_male\", \"median_age\",\n",
    "                         \"percent_62yearsandover\",\n",
    "                         \"percent_white\", \"percent_black\",\n",
    "                         \"percent_asian\", \"median_income\",\n",
    "                         \"employ_20to64\", \"highschool\", \"bach_orhigher\",\n",
    "                         \"percent_hispanicorlatino\"]\n",
    "binary_variables_names = [\"sex\",\"g2000\", \"g2002\", \"p2000\", \"p2002\", \"p2004\"]\n",
    "scaled_cts_covariates = StandardScaler().fit_transform(df[cts_variables_names])\n",
    "binary_covariates = df[binary_variables_names]\n",
    "d = pd.DataFrame(np.concatenate((scaled_cts_covariates, binary_covariates), axis=1), \n",
    "                        columns=cts_variables_names+binary_variables_names, index=df.index)\n",
    "d[\"W\"] = df[\"treat_neighbors\"]\n",
    "d[\"Y\"] = df[\"outcome_voted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    81803\n",
       "1    38196\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['Y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: ATE for RCT\n",
    "All individuals in this experiment had an equal probability of being assigned to the treatment group, so the difference-of-means will be a reasonable estimator for the average treatment effect (ATE).  Write a function that computes the difference-of-means estimator for the treatment effect, along with an approximate 95% confidence interval.  Apply it to the dataset d computed above and report the results.  Save the estimate of the ATE and the radius of the confidence interval for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_of_means(d, alpha=0.05):\n",
    "    # TODO\n",
    "    treated_Y = d[d['W']==1]['Y']\n",
    "    control_Y =  d[d['W']==0]['Y']\n",
    "    \n",
    "    ate = treated_Y.mean()-control_Y.mean()\n",
    "    var_dom = treated_Y.var()/len(treated_Y)+control_Y.var()/len(control_Y)\n",
    "    std_dom = var_dom**.5\n",
    "    \n",
    "    ate_CI_radius = norm.ppf(1-alpha/2)*std_dom\n",
    "    return ate,ate_CI_radius\n",
    "\n",
    "\n",
    "ate, ate_CI_radius = get_diff_of_means(d, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 0.08639696096960969\n",
      "ATE Radius: 0.007337282328514592\n"
     ]
    }
   ],
   "source": [
    "print('ATE:',ate)\n",
    "print('ATE Radius:',ate_CI_radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: ATE estimation with known confounders\n",
    "In this problem we're going to take a relatively small and biased subsample of our full dataset and try to use that to estimate the ATE. Our approach is to reduce ATE estimation to estimating a mean in the MAR setting.\n",
    "\n",
    "Below we give a function that takes a biased sample using an approach similar to [this one](https://gsbdbi.github.io/ml_tutorial/ate_tutorial/ate_tutorial.html#introducing_sampling_bias). The details aren't important for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6583804865040542\n",
      "0.4248535404461704\n",
      "We've sampled 2740 instances out of 119999.\n"
     ]
    }
   ],
   "source": [
    "likely_voters =  (((d['g2002'] == 1) & (d['sex'] == 0)) | (d['p2004'] == 1))\n",
    "unlikely_voters_control = (~likely_voters) & (d[\"W\"] == 0)\n",
    "likely_voters_treatment = likely_voters & (d[\"W\"] == 1)\n",
    "print(likely_voters.mean())\n",
    "def get_biased_sample(d, overall_subsample_rate=.03, bias_rate=.4, rng=default_rng(0)):\n",
    "    keep_prob = overall_subsample_rate * np.ones(len(d))\n",
    "    keep_prob[unlikely_voters_control] *= bias_rate\n",
    "    keep_prob[likely_voters_treatment] *= bias_rate\n",
    "    keep = rng.random(len(d)) <= keep_prob\n",
    "    d_bias = d[keep]\n",
    "    \n",
    "    return d_bias\n",
    "\n",
    "d_bias = get_biased_sample(d)\n",
    "new_likely_voters =  (((d_bias['g2002'] == 1) & (d['sex'] == 0)) | (d['p2004'] == 1))\n",
    "print(new_likely_voters.mean())\n",
    "print(f\"We've sampled {len(d_bias)} instances out of {len(d)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "Below we provide the function get_basic_MAR_estimators, which computes the complete-case mean and the IPW mean in the MAR setting with known propensity scores. You are to complete the get_ATE_estimators function below so that it *uses the get_basic_MAR_estimators function* and computes one ATE estimate for each type of estimator produced by get_basic_MAR_estimators (i.e. the complete-case mean and the IPW mean).  Use logistic regression to estimate the propensity scores from the data provided to the function.  Run the existing code block below to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_MAR_estimators(Y, X, pi, R):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        Y (pd.Series): the measurement of the outcome Y_i\n",
    "        X (pd.DataFrame): covariates, rows correspond to entries of Y\n",
    "        R (pd.Series): boolean series indicating whether Y was observed\n",
    "        pi (pd.Series): propensity scores corresponding to observations\n",
    "    \n",
    "    Returns:\n",
    "        dict of estimator names and estimates for EY\n",
    "    \"\"\"\n",
    "    est = {}\n",
    "    n = len(Y)\n",
    "    ## All the estimators below assume we know the pi (i.e. \"missing by design\")\n",
    "    est[\"mean\"] = np.mean(Y[R])\n",
    "    est[\"ipw_mean\"] = np.sum(Y[R] / pi[R]) / n\n",
    "    \n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ATE_estimators(Y, X, W, get_MAR_estimators=get_basic_MAR_estimators):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        Y (pd.Series): the measurement of the outcome Y_i\n",
    "        X (pd.DataFrame): covariates, rows correspond to entries or Y\n",
    "        W (pd.Series): 0/1 series indicating control (0) or treatment (1) assignment\n",
    "        get_MAR_estimators: function behaving like get_basic_MAR_estimators above\n",
    "    \n",
    "    Returns:\n",
    "        dict of ATE estimator names and estimates, same format as for get_MAR_estimators\n",
    "    \"\"\"   \n",
    "    # TODO\n",
    "    prop_model = LogisticRegression()\n",
    "    prop_model.fit(X,W)\n",
    "    propensities = prop_model.predict_proba(X)\n",
    "    #print(model.classes_)\n",
    "    pi_control, pi_treated = propensities[:,0],propensities[:,1]\n",
    "    \n",
    "    R_treated = W==1\n",
    "    R_control = W==0\n",
    "    \n",
    "    est_control = get_MAR_estimators(Y,X,pi_control,R_control)\n",
    "    est_treated = get_MAR_estimators(Y,X,pi_treated,R_treated)\n",
    "    \n",
    "    ate = {}\n",
    "    for key in est_control:\n",
    "        ate[key] = est_treated[key] - est_control[key]\n",
    "    return ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 0.03636661211129294, 'ipw_mean': 0.14150098309907239}\n"
     ]
    }
   ],
   "source": [
    "## Run the following and report \n",
    "d_bias = get_biased_sample(d, overall_subsample_rate=.03, bias_rate=.4, rng=default_rng(0))\n",
    "X = d_bias.drop(columns=['W','Y'])\n",
    "ate_est = get_ATE_estimators(Y=d_bias[\"Y\"], X=X, W=d_bias[\"W\"], get_MAR_estimators=get_basic_MAR_estimators)\n",
    "print(ate_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "In this part we significantly expand our ATE estimators and see how they perform over repeated trials.  Complete get_MAR_estimators below to include \n",
    "- Self-normalized IPW mean\n",
    "- Linear regression imputation\n",
    "- IPW linear regression imputation (as defined in lecture)\n",
    "- IW linear regression imputation (as defined in lecture)\n",
    "- Augmented IPW using linear regression\n",
    "- [Optional (not for credit): nonlinear regression, or any other variations you'd like to try]\n",
    "\n",
    "Run the code below to assess performance of these estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MAR_estimators(Y, X, pi, R):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        Y (pd.Series): the measurement of the outcome Y_i\n",
    "        X (pd.DataFrame): covariates, rows correspond to entries or Y\n",
    "        R (pd.Series): boolean series indicating whether Y was observed\n",
    "        pi (pd.Series): propensity scores corresponding to observations\n",
    "    \n",
    "    Returns:\n",
    "        dict of estimator names and estimates for EY\n",
    "    \"\"\"\n",
    "    est = {}\n",
    "    n = len(Y)\n",
    "    ## All the estimators below assume we know the pi (i.e. \"missing by design\")\n",
    "    est[\"mean\"] = np.mean(Y[R])\n",
    "    est[\"ipw_mean\"] = np.sum(Y[R] / pi[R]) / n\n",
    "    est[\"sn_ipw_mean\"] = np.sum(Y[R]/pi[R])/np.sum(1/pi[R])\n",
    "    \n",
    "    #Linear Imputation\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[R],Y[R],)\n",
    "    est[\"impute_lr\"] = np.concatenate([model.predict(X[~R]),Y[R]]).mean()\n",
    "    \n",
    "    #Importance Weighting on the complete cases to incomplete cases, IW_linear\n",
    "    weights = (1-pi)/pi\n",
    "    model = LinearRegression()\n",
    "    #fit on complete cases, but predict on missing cases, then average to get estimate for Y\n",
    "    model.fit(X[R],Y[R],weights[R])\n",
    "    est[\"impute_iw_lr\"] = np.concatenate([model.predict(X[~R]),Y[R]]).mean()\n",
    "    \n",
    "    #Importance Weighting on the complete cases to ALL cases, IPW_Linear\n",
    "    weights = 1/pi\n",
    "    model = LinearRegression()\n",
    "    #Fit on complete cases, predict on ALL cases, then average to get estimate for Y\n",
    "    model.fit(X[R],Y[R],weights[R])\n",
    "    est[\"impute_ipw_lr\"] = model.predict(X).mean()\n",
    "    \n",
    "    #Augmented IPW\n",
    "    #F is fit on the complete cases to predict on ONLY complete cases\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[R],Y[R],)\n",
    "    f_x = model.predict(X)\n",
    "    \n",
    "    est['a_ipw'] = ( np.sum( (Y*R)/pi - (f_x*R)/pi + f_x) )/n\n",
    "    \n",
    "    #Augmented IPW w/ IW\n",
    "    #F is fit on the complete cases to predict on ALL Cases\n",
    "    weights = 1/pi\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[R],Y[R],weights[R])\n",
    "    f_x = model.predict(X)\n",
    "    \n",
    "    est['a_ipw_IW'] = ( np.sum( (Y*R)/pi - (f_x*R)/pi + f_x) )/n\n",
    "    \n",
    "    ## TODO\n",
    "    return est\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 0.03636661211129294, 'ipw_mean': 0.14150098309907239, 'sn_ipw_mean': 0.10043918758877857, 'impute_lr': 0.06586463436122314, 'impute_iw_lr': 0.07797247353871739, 'impute_ipw_lr': 0.07930786641863158, 'a_ipw': 0.09223360039649053, 'a_ipw_IW': 0.07930786641863136}\n"
     ]
    }
   ],
   "source": [
    "## Run the following and report \n",
    "d_bias = get_biased_sample(d, overall_subsample_rate=.03, bias_rate=.4, rng=default_rng(0))\n",
    "X = d_bias.drop(columns=['W','Y'])\n",
    "ate_est = get_ATE_estimators(Y=d_bias[\"Y\"], X=X, W=d_bias[\"W\"], get_MAR_estimators=get_MAR_estimators)\n",
    "print(ate_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator_stats(estimates, true_parameter_value=None):\n",
    "    \"\"\"\n",
    " \n",
    "     Args:\n",
    "        estimates (pd.DataFrame): each row corresponds to collection of estimates for a sample and\n",
    "            each column corresponds to an estimator\n",
    "        true_parameter_value (float): the true parameter value that we will be comparing estimates to\n",
    "            \n",
    "    Returns:\n",
    "        pd.Dataframe where each row represents data about a single estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    est_stat = []\n",
    "    for est in estimates.columns:\n",
    "        pred_means = estimates[est]\n",
    "        stat = {}\n",
    "        stat['stat'] = est\n",
    "        stat['mean'] = np.mean(pred_means)\n",
    "        stat['SD'] = np.std(pred_means)\n",
    "        stat['SE'] = np.std(pred_means) / np.sqrt(len(pred_means))\n",
    "        if true_parameter_value:\n",
    "            stat['bias'] = stat['mean'] - true_parameter_value\n",
    "            stat['RMSE'] = np.sqrt(np.mean((pred_means - true_parameter_value) ** 2))\n",
    "        est_stat.append(stat)\n",
    "\n",
    "    return pd.DataFrame(est_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(sampler, num_repeats=10,rng=default_rng(0)):\n",
    "    data_list = []\n",
    "    num_obs_list = []\n",
    "    for i in range(num_repeats): \n",
    "        d = sampler(rng)\n",
    "        X = d.drop(columns=['W','Y'])\n",
    "        ate_est = get_ATE_estimators(Y=d[\"Y\"], X=X, W=d[\"W\"], get_MAR_estimators=get_MAR_estimators)\n",
    "        data_list.append(ate_est)\n",
    "    results = pd.DataFrame(data_list)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(rng):\n",
    "    return get_biased_sample(d, overall_subsample_rate=.06, bias_rate=.4, rng=rng)\n",
    "rng = default_rng(0)\n",
    "results = run_experiments(sampler, num_repeats=500, rng=rng) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stat</th>\n",
       "      <th>mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>SE</th>\n",
       "      <th>bias</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.033087</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>-0.053310</td>\n",
       "      <td>0.056380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ipw_mean</td>\n",
       "      <td>0.134785</td>\n",
       "      <td>0.028370</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.048388</td>\n",
       "      <td>0.056091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sn_ipw_mean</td>\n",
       "      <td>0.101437</td>\n",
       "      <td>0.025028</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.029199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>impute_lr</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.022025</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.022043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>impute_iw_lr</td>\n",
       "      <td>0.086868</td>\n",
       "      <td>0.022254</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.022259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>impute_ipw_lr</td>\n",
       "      <td>0.086888</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.022267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a_ipw</td>\n",
       "      <td>0.087040</td>\n",
       "      <td>0.022588</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.022597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a_ipw_IW</td>\n",
       "      <td>0.086888</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.022267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stat      mean        SD        SE      bias      RMSE\n",
       "0           mean  0.033087  0.018351  0.000821 -0.053310  0.056380\n",
       "1       ipw_mean  0.134785  0.028370  0.001269  0.048388  0.056091\n",
       "2    sn_ipw_mean  0.101437  0.025028  0.001119  0.015040  0.029199\n",
       "3      impute_lr  0.087300  0.022025  0.000985  0.000903  0.022043\n",
       "4   impute_iw_lr  0.086868  0.022254  0.000995  0.000471  0.022259\n",
       "5  impute_ipw_lr  0.086888  0.022261  0.000996  0.000491  0.022267\n",
       "6          a_ipw  0.087040  0.022588  0.001010  0.000643  0.022597\n",
       "7       a_ipw_IW  0.086888  0.022261  0.000996  0.000491  0.022267"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_eval = get_estimator_stats(results, true_parameter_value=ate)\n",
    "results_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "You should see that the ATE estimate based on the IPW mean is significantly biased, and the bias seems to be driving most of the RMSE. In class we showed that the IPW mean is an unbiased estimator for EY in the MAR setting, and the corresponding ATE estimator is also unbiased.  Why are we seeing bias in this experiment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing bias because of covariate shift.\n",
    "\n",
    "Previously, we got a biased sample of the data in the function `get_biased_sample`. If we examine the code/method/link given, we see that we are introducing a _sampling bias based on the covariate $X$ and the treatment variable $W$_. In particular, this means that the specifically the value of $X$ has an influence on whether or the the data point shows up in return of the `get biased sample` function. \n",
    "\n",
    "Our results generated in part B of this problem used the \"true value of ATE\" as was calculated in problem 1. The results in problem 1 were generated using the full dataset, which has a certain distribution of covariates $X$. In Problem 2, we dropped some likely treated voters, and dropped some unlikely non-treated voters. Note that how likely voters were defined `likely_voters =  (((d['g2002'] == 1) & (d['sex'] == 0)) | (d['p2004'] == 1))` depends only on the covariates $X$.\n",
    "\n",
    "Now, the code in the following cell shows that the ratio of likely voters before -> after dropping was 0.658 -> 0.424.\n",
    "\n",
    "It is likely that the CATE(likely voter) is different than the CATE(non-likely voter), though we do not show this explicity. \n",
    "\n",
    "Both those statements imply that there is a significant degree of covariate shift, which affects the estimate for the full ATE.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of likely voters before dropping: 0.6583804865040542\n",
      "Proportion of likely voters after dropping: 0.4248535404461704\n",
      "We've sampled 2740 instances out of 119999.\n"
     ]
    }
   ],
   "source": [
    "likely_voters =  (((d['g2002'] == 1) & (d['sex'] == 0)) | (d['p2004'] == 1))\n",
    "unlikely_voters_control = (~likely_voters) & (d[\"W\"] == 0)\n",
    "likely_voters_treatment = likely_voters & (d[\"W\"] == 1)\n",
    "print('Proportion of likely voters before dropping:', likely_voters.mean())\n",
    "def get_biased_sample(d, overall_subsample_rate=.03, bias_rate=.4, rng=default_rng(0)):\n",
    "    keep_prob = overall_subsample_rate * np.ones(len(d))\n",
    "    keep_prob[unlikely_voters_control] *= bias_rate\n",
    "    keep_prob[likely_voters_treatment] *= bias_rate\n",
    "    keep = rng.random(len(d)) <= keep_prob\n",
    "    d_bias = d[keep]\n",
    "    \n",
    "    return d_bias\n",
    "\n",
    "d_bias = get_biased_sample(d)\n",
    "new_likely_voters =  (((d_bias['g2002'] == 1) & (d['sex'] == 0)) | (d['p2004'] == 1))\n",
    "print('Proportion of likely voters after dropping:', new_likely_voters.mean())\n",
    "print(f\"We've sampled {len(d_bias)} instances out of {len(d)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Bootstrap confidence intervals for ATE\n",
    "Let's now consider a more realistic scenario, in which we only have a single sample to work with.  In this case it's very important to be able to include some form of uncertainty measure with our estimates.  In this problem we'll do this using the normal approximated bootstrap confidence intervals described in our module on CATEs. However, in this setting, rather than estimating the CATE, we're just estimating the ATE. \n",
    "\n",
    "#### Part A\n",
    "Complete the function get_stratified_bootstrap_CI to generate 95% normal approximated bootstrap confidence intervals for each of the ATE estimators we've developed above. Execute the code below to test the function and produce the results as a table and in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_stats(boot_estimates, full_estimates, alpha=0.05):\n",
    "    est_stat = []\n",
    "    signif_level = -norm.ppf(alpha/2)\n",
    "    for est in full_estimates:\n",
    "        est_boot = np.array(boot_estimates[est])\n",
    "        stat = {}\n",
    "        stat['estimator'] = est\n",
    "        stat['estimate'] = full_estimates[est]\n",
    "        #stat['mean_boot'] = np.mean(est_boot)\n",
    "        stat['SD'] = np.std(est_boot)\n",
    "        stat['CI_radius'] = signif_level * stat['SD']\n",
    "        stat['lower_ci'] = stat['estimate'] - stat['CI_radius']\n",
    "        stat['upper_ci'] = stat['estimate'] + stat['CI_radius']\n",
    "        est_stat.append(stat)\n",
    "\n",
    "    return pd.DataFrame(est_stat)\n",
    "\n",
    "\n",
    "def get_stratified_bootstrap_CI(Y, X, W, estimators, num_bootstrap=10, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        pd.Dataframe with\n",
    "            column \"estimator\" with the name of the estimator,\n",
    "            column \"estimate\" which is the estimate based on (Y,X,W)\n",
    "                which serves as the center of our bootstrap confidence intervals\n",
    "            column \"CI_radius\" which is the radius of the confidence interval\n",
    "            (additional columns may be included as you wish)\n",
    "    \"\"\"    \n",
    "    #TODO\n",
    "    n = len(Y)\n",
    "    all_estimates = defaultdict(list)\n",
    "    est_list = []\n",
    "    treated_idx = (W==1).index\n",
    "    control_idx = (W==0).index\n",
    "    for ii in range(num_bootstrap):\n",
    "        treated_idx_to_use = np.random.choice(treated_idx,size = len(Y),replace=True)\n",
    "        control_idx_to_use = np.random.choice(control_idx,size = len(Y),replace=True)\n",
    "        idx_to_use = np.concatenate([treated_idx_to_use,control_idx_to_use])\n",
    "        Y_r = Y[idx_to_use] \n",
    "        X_r = X.loc[idx_to_use]\n",
    "        W_r = W[idx_to_use] \n",
    "        \n",
    "        est = get_ATE_estimators(Y_r, X_r, W_r, get_MAR_estimators=get_MAR_estimators)\n",
    "        est_list.append(est)\n",
    "    true_estimates = get_ATE_estimators(Y,X,W,get_MAR_estimators=get_MAR_estimators)\n",
    "    bootstrap_est=pd.DataFrame(est_list)\n",
    "    #display(bootstrap_est)\n",
    "    \n",
    "    return_df = get_bootstrap_stats(bootstrap_est,true_estimates,alpha=0.05)\n",
    "\n",
    "    return return_df\n",
    "\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 95% confidence interval for the ATE based on the full dataset is 0.08639696096960969+/-0.007337282328514592\n",
      "       estimator  estimate        SD  CI_radius  lower_ci  upper_ci\n",
      "0           mean  0.032720  0.012969   0.025418  0.007302  0.058138\n",
      "1       ipw_mean  0.138417  0.021569   0.042274  0.096143  0.180690\n",
      "2    sn_ipw_mean  0.093061  0.018340   0.035945  0.057116  0.129006\n",
      "3      impute_lr  0.084381  0.015213   0.029818  0.054563  0.114199\n",
      "4   impute_iw_lr  0.081245  0.015290   0.029969  0.051276  0.111214\n",
      "5  impute_ipw_lr  0.080742  0.015349   0.030084  0.050658  0.110825\n",
      "6          a_ipw  0.076795  0.016389   0.032122  0.044673  0.108918\n",
      "7       a_ipw_IW  0.080742  0.015349   0.030084  0.050658  0.110825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'mean'),\n",
       " Text(0, 1, 'ipw_mean'),\n",
       " Text(0, 2, 'sn_ipw_mean'),\n",
       " Text(0, 3, 'impute_lr'),\n",
       " Text(0, 4, 'impute_iw_lr'),\n",
       " Text(0, 5, 'impute_ipw_lr'),\n",
       " Text(0, 6, 'a_ipw'),\n",
       " Text(0, 7, 'a_ipw_IW'),\n",
       " Text(0, 8, 'Full data estimate')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = default_rng(8)\n",
    "d_samp = sampler(rng) # our single biased sample\n",
    "X = d_samp.drop(columns=['W','Y'])\n",
    "estimators = lambda Y, X, W: get_ATE_estimators(Y, X, W, get_MAR_estimators=get_MAR_estimators)\n",
    "ci = get_stratified_bootstrap_CI(Y=d_samp[\"Y\"], X=X, W=d_samp[\"W\"], estimators=estimators, num_bootstrap=500)\n",
    "print(f\"A 95% confidence interval for the ATE based on the full dataset is {ate}+/-{ate_CI_radius}\")\n",
    "print(ci)\n",
    "to_plot = ci[['estimator','estimate','CI_radius']]\n",
    "to_plot = to_plot.append({'estimator':'Full data estimate','estimate':ate,'CI_radius':ate_CI_radius}, ignore_index=True)\n",
    "fig, ax =plt.subplots(1,1, figsize=(5,7))\n",
    "ax.errorbar(to_plot['estimate'], np.arange(len(to_plot)), \\\n",
    "            xerr=to_plot['CI_radius'], \n",
    "            fmt='o', elinewidth=3, capsize=5)\n",
    "ax.grid('on')\n",
    "ax.set_yticks(np.arange(len(to_plot)))\n",
    "ax.set_yticklabels(to_plot[\"estimator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
